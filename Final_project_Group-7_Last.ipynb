{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS8008 Final Project: Pairwise Multi-Class Document Classification for Semantic Relations\n",
    "\n",
    "#### Members Names: Masuk Ul Islam, Mohammad Wahidul Islam Murad\n",
    "\n",
    "#### Members Emails: masuk.islam@ryerson.ca, m2murad@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "To cope with the ever-emerging information overload, digital libraries employ literature recommender systems. These\n",
    "systems recommend related documents with the help of similarity measures, which often only distinguish between similar and dissimilar documents.\n",
    "\n",
    "The objective is to design the model for finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, a series of techniques, such as GloVe, Paragraph-Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. The experiments performed on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. It remains unclear to which of the many facets the similarity relates.\n",
    "\n",
    "Based on the the findings we can deduce that classifying semantic relations between documents is a solvable task and motivates the development of recommender systems based on the evaluated techniques. The discussions in the paper serves as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "While other NLP tasks, like relation extraction, deal with relations, they are not concerned with semantic relations between documents.The choice of similarity measures is widely used and works well on a wide range of documents, but has no theoretical basis. A similarity measure is a function which computes the degree of similarity between a pair of vectors or documents. Since queries and documents are both vectors, a similarity measure can represent the similarity between two documents, two queries, or one document and one query. There are a large number of similarity measures proposed in the past, because the best similarity measure doesn't exist yet. \n",
    "\n",
    "The other limitations that exists at present are:\n",
    "\n",
    "Synonyms: separate words that have the same meaning. (E.g. ‘car’ & ‘automobile’). They tend to reduce recall.\n",
    "Polysems: words with multiple meanings (E.g. ‘Java’). They tend to reduce precision. The problem is more general: there is a disconnect between topicsand words\n",
    "\n",
    "TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc. It cannont capture semantics (e.g. as compared to topic models, word embeddings).\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "The task of finding semantic document relations is implemented as a multi-class classification of document pairs. The paper demonstrated the viability of this approach with a new proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define semantic relations among these articles. In an empirical study, implemented six different models AvgGloVe, Doc2vec, Siamese BERT, Siamese XLNet, vanilla BERT and vanilla XLNet, and evaluate them under\n",
    "different settings regarding the concatenation method and sequence length. The evaluation indicates a sequence length of\n",
    "512 tokens as the best performing sequence limit for the Siamese and vanilla Transformer models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In this project we tried to mimic the implementation based on the paper https://arxiv.org/pdf/2003.09881\n",
    "\n",
    "Bär et al. [7] discuss the notion of similarity between texts in the context of NLP. They express that while text similarity is present in many NLP tasks, the similarity is often ill-defined and used as an “umbrella term covering quite different phenomena”. Bär et al. [7] formalize text similarity and suggest content, structure, and style as the major dimensions inherent to texts. Structure and style are not actively being accounted for. Therefore, we focused only on the content. With the classification of semantic document relations, we intend to tailor recommendations depending on specific information needs. For example, we could provide either recommendations focusing on a particular relation class or diverse recommendations from multiple relation classes.\n",
    "\n",
    "The Transformer architecture allowed the efficient unsupervised pretraining of language models and led to significant improvements in many NLP benchmarks. Akkalyoncu Yilmaz et al. [3] applied BERT to an information retrieval system for\n",
    "an end-to-end search over large document collections. Despite their success in NLP, Transformers have gained little attention in the recommender system community so far.  In our experiments, we utilize the article text and learn the document relation using a multilayer perceptron (MLP).\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Malte O [1] | find the semantic relation between documents by treating the two as a pairwise document classification | digital library from Wikipedia through Wikidata | Doc2Vec model with the lowest F1 score of 0.83\n",
    "| Ashutosh A, Achyudh R, Raphael T, and Jimmy Lin [2] | Here computational expense associated with BERT inference is addressed by leveraging distill knowledge from BERT_large to small bidirectional LSTMs| Reuters-21578 , arXiv Academic Paper dataset , IMDB reviews, and Yelp 2014 reviews | Implementaion of the same model using Neural Network Architecture\n",
    "| Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, and Jimmy Lin [3] | Here BIRCH, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit is implemented to demonstrate end-to-end search over large document collections | Shot Social Media Posts | precision at rank 30 is 70.06%\n",
    "| power, jchen, trishank, lakshmi [4] | This is a simple feature extraction algorithm that can achieve high document classification accuracy in the context of development-centric topics| 10000 random documents from the English wikipedia corpus| when classification tasks are restricted to relatively narrow topics of interest, we can achieve near perfect precision and recall\n",
    "|Chaitanya Anne, Avdesh Mishra, Md Tamjidul Hoque, Shengru Tu [5]|Multiclass patent document classification by using array of methods like kNN, SVM and two tree based classification algorithm: Random Forest and J48|NASA’s predefined patent documents| fusion technique could further be implemented to improve the accuracy of the multiclass classification problem\n",
    "|Tillmann Donicke, Florian Lux, and Matthias Damaschk [6]|Methods to improve the performance of text classification on data that is difficult to classify due to a large number of unbalanced classes with noisy examples | songtexts from Kaggle |Implement CNNs and RNNs for the same set for better result\n",
    "|Daniel Bar, Torsten Zesch, and Iryna Gurevych [7]|text similarity with respect to its definition and the datasets used for evaluation |Microsoft Paraphrase Corpus |Recognizing paraphrases is a very hard task that cannot simply be tackled by computing text similarity, as sharing similar content is a necessary, but not a sufficient condition for detecting paraphrases\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Here, we combine the ideas of relation extraction, document classification, and document similarity to classify the semantic\n",
    "relation of document pairs. Given a seed document ds , we are interested in finding a target document dt that shares the semantic relation r(i) with d(s) . The task is to model to find the relation r of a document pair (ds ,dt ) as a pairwise multi-class document classification problem.\n",
    "\n",
    "To evaluate the presented techniques, we build a dataset using Wikipedia and Wikidata repositories to illustrate our problem. Wikipedia articles are the seed and target documents, while Wikidata properties provide the semantic relations between a document pair. The following figure shows one example from the dataset:\n",
    "\n",
    "![Alternate text ](semanticRelation.png \"Semantic Relation Explained, location is simply the directory of the notebook\")\n",
    "\n",
    "To classify the semantic relations of document pairs we are implementing deep contextual language models from BERT. In the process a novel dataset composed of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic relation of these articles are shared.\n",
    "\n",
    "As one of the main goal of this paper is to explore the multi-class classification problem, so we ensured that the same pair of documents did not share different labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells\n",
    "from models.transformers import JointBERT\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from wiki.utils import get_text_from_wikipedia\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from experiments import Experiment\n",
    "from wiki.data_helpers import JointBERTWikiDataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells\n",
    "if not torch.cuda.is_available():\n",
    "  raise ValueError('CUDA is not available. Please enable GPU support under: Runtime > Change runtime')\n",
    "\n",
    "# Load experimental settings\n",
    "exp_dir = '4fold_results/1/wiki.bert_base__joint__seq512'\n",
    "\n",
    "with open(os.path.join(exp_dir, 'experiment.pickle'), 'rb') as f:\n",
    "  _exp = pickle.load(f)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(_exp['data_helper_params']['labels'] + ['none'])\n",
    "\n",
    "# Init model\n",
    "model = JointBERT(bert_model_path='bert-base-cased', bert_cls=BertModel, prob='sigmoid', labels_count=_exp['model_params']['labels_count'])\n",
    "model.load_state_dict(torch.load(os.path.join(exp_dir, 'model_weights')))\n",
    "\n",
    "#model = BertModel.from_pretrained('bert-base-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells\n",
    "# Retrieve article text from Wikipedia API\n",
    "seed_text = get_text_from_wikipedia(seed_title)\n",
    "target_text = get_text_from_wikipedia(target_title)\n",
    "\n",
    "# Convert to model input\n",
    "seed_ids = tokenizer.convert_tokens_to_ids((tokenizer.tokenize(seed_title) + tokenizer.tokenize(seed_text))[:250])\n",
    "target_ids = tokenizer.convert_tokens_to_ids((tokenizer.tokenize(target_title) + tokenizer.tokenize(target_text))[:250])\n",
    "\n",
    "pair_ids = [torch.tensor(tokenizer.build_inputs_with_special_tokens(seed_ids, target_ids))]\n",
    "token_type_ids = [torch.tensor(tokenizer.create_token_type_ids_from_sequences(seed_ids, target_ids))]\n",
    "\n",
    "pair_ids = pad_sequence(pair_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=1)\n",
    "\n",
    "masks = torch.tensor([[float(i > 0) for i in ii] for ii in pair_ids])\n",
    "\n",
    "# Prediction\n",
    "with torch.no_grad():\n",
    "  ys_pred = model(pair_ids, masks, token_type_ids).numpy()\n",
    "  label_pred = le.inverse_transform([ys_pred.argmax()])[0]\n",
    "    \n",
    "\n",
    "# Probabilites for other classes\n",
    "df = pd.DataFrame(dict(label=le.classes_.tolist(), probability=ys_pred.flatten().round(3).tolist()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Our findings suggest that pairwise document classification is a solvable task using existing techniques. One of the biggest challange would be to massage the data . The study suggests that a system can be built that enables users to explore scientific literature in an analogical manner. For instance, users could retrieve other research papers with a similar methodology but different result. To develop such a system, the Open Research Knowledge Graph could be utilized as the scientific equivalence of Wikidata, while research paper from any open digital library, e.g., arXiv, would correspond to Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Malte Ostendorff, Terry Ruas, Moritz Schubotz, Georg Rehm, Bela Gipp, Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles, JCDL 2020\n",
    "\n",
    "[2]:  Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin, DocBERT: BERT for Document Classification, 2019\n",
    "\n",
    "[3]:  Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, Jimmy Lin , Applying BERT to Document Retrieval with Birch, EMNLP 2019\n",
    "\n",
    "[4]:  Russell Power, Jay Chen, Trishank Karthik, Lakshminarayanan Subramanian, Document classification for focused topics , AAAI Spring Symposium Series 2010\n",
    "\n",
    "[5]:  Chaitanya Anne, Avdesh Mishra, Md Tamjidul Hoque, Shengru Tu , Multiclass patent document classification ,Artif. Intell. Research 2018\n",
    "\n",
    "[6]:  Matthias Damaschk, Tillmann Dönicke, Florian Lux , Multiclass Text Classification on Unbalanced, Sparse and Noisy Data ,NoDaLiDa 2019\n",
    "\n",
    "[7]:  Bär, Daniel & Zesch, Torsten & Gurevych, Iryna. A Reflective View on Text Similarity.. International Conference Recent Advances in Natural Language Processing, RANLP. 515-520, 2011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
